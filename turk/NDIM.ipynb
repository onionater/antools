{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pylab inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys \n",
      "sys.path.append('/Users/amyskerry/Dropbox/antools/')\n",
      "import csv\n",
      "import aeslazy as asl\n",
      "import numpy as np\n",
      "from itertools import *\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import scipy.io\n",
      "from sklearn import svm, cluster, decomposition\n",
      "import dealwithNDIMdata_funcs as ndim\n",
      "#import dealwithNDEdata as nde\n",
      "import analyzeNDE as nde_data\n",
      "#import itertools"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "checkout NDE data..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "resultsfile='/Users/amyskerry/documents/projects/turk/NDE_dim/data/NDE_data/sqldata/NDEdl.csv'\n",
      "#resultsfile='/Users/amyskerry/documents/projects/turk/NDE_dim2/data/NDE_data/sqldata/NDEdl_combined.csv' #contains NDEdl.csv and the first row of the two woops (with checks manually corrected since these subjects didn't have Neutral option)\n",
      "\n",
      "#hardcoding\n",
      "#checkquestions=(201,202)\n",
      "checkquestions=(86,87)\n",
      "expectedanswers=('Neutral', 'Neutral')\n",
      "#orderedemos=['Grateful', 'Joyful','Hopeful','Excited','Proud','Impressed','Content','Nostalgic', 'Surprised','Lonely', 'Furious','Terrified','Apprehensive','Annoyed', 'Guilty', 'Disgusted','Embarrassed','Devastated', 'Disappointed', 'Jealous']\n",
      "orderedemos=['Grateful', 'Joyful','Hopeful','Proud','Impressed','Content','Nostalgic', 'Surprise','Lonely', 'Angry','Afraid','Apprehensive','Annoyed', 'Guilty', 'Disgusted','Embarrassed','Sad', 'Disappointed']\n",
      "maxN=10 #blacklist questions with this many or more\n",
      "inclusioncols={'submission_date': (lambda inputval: inputval not in ('NULL',))} #key=column, value=function returning whether given item is a keeper\n",
      "\n",
      "#main\n",
      "[varnames,datamatrix]=nde_data.extractdata(resultsfile)\n",
      "checkfailers=nde_data.findcheckfailers(datamatrix, varnames, checkquestions, expectedanswers)\n",
      "incfailers=nde_data.testinclusioncrit(datamatrix, varnames, inclusioncols)\n",
      "excl_list=[cf*incfailers[cfn] for cfn,cf in enumerate(checkfailers)]\n",
      "[labels,answers,correctness,responses,counts]=nde_data.scoreitems(varnames,datamatrix,checkquestions,excl_list)\n",
      "stimmeans=list(np.nanmean(correctness,0))\n",
      "f,axes=plt.subplots(2)\n",
      "axes[0].bar(range(len(stimmeans)), stimmeans);axes[0].set_xlim([0,len(stimmeans)]);axes[0].set_title('accuracies')\n",
      "axes[1].bar(range(len(stimmeans)), counts);axes[1].set_xlim([0,len(stimmeans)]);axes[1].set_title('counts')\n",
      "[emonames, emoaccuracies, emoerrorcounts, emoerrors]=nde_data.condenseaccuracies(varnames, stimmeans,answers,checkquestions, responses,orderedemos)\n",
      "blacklist=[l for ln, l in enumerate(labels) if counts[ln]>maxN]\n",
      "nde_data.printdeets(labels, counts, stimmeans, blacklist, maxN)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "NDIM data..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "global othercols, excludecols, othercols\n",
      "filename='/Users/amyskerry/NDE_dimdl.csv'\n",
      "#important columns\n",
      "othercols=['subjid', 'rownum','submission_date', 'city','country','age','gender','thoughts']\n",
      "suffix='vonly'\n",
      "smallfig=[3,2]\n",
      "screesize=[4,3]\n",
      "matrixsize=[7,6]\n",
      "largefig=[12,8]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "alldims=['expectedness', 'pleasantness', 'goal_consistency', 'fairness', 'agent_cause', 'agent_intention', 'self_cause', 'close_others', 'control', 'fixing', 'moral', 'confidence', 'suddenness', 'familiarity', 'past_present', 'certainty', 'coping', 'mental_states', 'others_knowledge', 'bodily_disease', 'people', 'relevance', 'freedom', 'pressure', 'consequences', 'safety', 'self_involvement']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "if suffix=='allvars':\n",
      "    excludecols=[]\n",
      "elif suffix=='nv':\n",
      "    excludecols=['pleasantness', 'goal_consistency', 'safety']\n",
      "elif suffix=='vonly':\n",
      "    valence=['pleasantness', 'goal_consistency', 'safety']\n",
      "    excludecols=[i for i in alldims if i not in valence]\n",
      "    #excludecols=['expectedness','fairness', 'agent_cause', 'agent_intention', 'self_cause', 'close_others', 'control', 'fixing', 'moral', 'confidence', 'suddenness', 'familiarity', 'past_present', 'certainty', 'coping', 'mental_states', 'others_knowledge', 'bodily_disease', 'people', 'relevance', 'freedom', 'pressure', 'consequences', 'self_involvement']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#I want to specify an intuitive ordering for visualizing dimensions\n",
      "newdimordering=['familiarity','expectedness','certainty','suddenness','pleasantness', 'goal_consistency',  'control', 'fixing','self_cause','agent_cause', 'agent_intention', 'coping','pressure', 'freedom', 'moral','fairness', 'past_present', 'bodily_disease','consequences', 'safety', 'close_others','people','mental_states', 'others_knowledge', 'confidence','relevance', 'self_involvement']\n",
      "newdimordering=[el for el in newdimordering if el not in excludecols]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "find useable subejcts and define vectors of class labels:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "[subjects, dims,emolabelmapping]=ndim.extractdata(filename, excludecols, othercols, newdimordering)\n",
      "keepers=[subj for subj in subjects if subj.passedcheck()]\n",
      "keeperlabels=[keep.label for keep in keepers]\n",
      "keeperemos=[keep.emo for keep in keepers]\n",
      "qlabels=sorted(set(keeperlabels), key=keeperlabels.index)#shouldn't matter that these are ordered, but just in case\n",
      "emolabels=sorted(set(keeperemos),key=keeperemos.index)#shouldn't matter that these are ordered, but just in case\n",
      "keepers=ndim.assignCVfolds(keepers,qlabels,emolabels)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "actualy emolabels are randomly orded, here use manually sorted labels:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "[orderedlabels]=ndim.orderlists(emolabels,qlabels,keepers,orderemos)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "compute item, emo, and dim avgs:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "[itemavgs,itemlabels, dimlabels, itememos]=ndim.getitemavgs(keepers,orderedlabels, dims)\n",
      "[emoavgs, emolabels, dimlabels]=ndim.getemoavgs(keepers,orderedemos, dims)\n",
      "dimavgs=np.array(itemavgs).T\n",
      "ndim.plotweightmatrix('emo-avgs x dimensions', dimlabels, emolabels, emoavgs, 'emosxdims_'+suffix+'.png', figuresize=[10,6],cmin=0, cmax=10)\n",
      "ndim.plotcorrmatrix('item-wise correlations (of avg item vectors of dim scores)', orderedlabels, itemavgs, 'items_'+suffix+'.png', figuresize=matrixsize)\n",
      "ndim.plotcorrmatrix('emo-wise correlations (of avg emo vectors of dim scores)', orderedemos, emoavgs, 'emos_'+suffix+'.png',figuresize=matrixsize)\n",
      "ndim.plotcorrmatrix('dim-wise correlations (of avg dimension vectors of item avgs)', dimlabels, dimavgs, 'dims_'+suffix+'.png',figuresize=matrixsize)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def reduce2subset(subset,itavgs, ilabels, iemos, emavgs, elabels):\n",
      "    reduceditavgs=[item for itemn,item in enumerate(itavgs) if iemos[itemn] in subset]\n",
      "    reducedilabels=[item for itemn,item in enumerate(ilabels) if iemos[itemn] in subset]\n",
      "    reducediemos=[item for item in iemos if item in subset]\n",
      "    reducedemavgs=[item for itemn,item in enumerate(emavgs) if elabels[itemn] in subset]\n",
      "    reducedelabels=[item for item in elabels if item in subset]\n",
      "    return reduceditavgs,reducedilabels,reducediemos, reducedemavgs, reducedelabels"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#temporarily limiting to subset of emos\n",
      "basicsubset=['Afraid', 'Joyful', 'Disgusted', 'Sad', 'Surprise', 'Angry']\n",
      "#[itemavgs, itemlabels, itememos, emoavgs, emolabels]=reduce2subset(basicsubset,itemavgs, itemlabels, itememos, emoavgs, emolabels)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "do clustering analysis with number of emotions imposed:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "k_means = cluster.KMeans(n_clusters=len(emolabels))\n",
      "k_means.fit(itemavgs)\n",
      "kclusters=k_means.labels_\n",
      "kclusters, itememos = zip(*sorted(zip(kclusters, itememos))) # zipping together, sorting, and unzipping\n",
      "for cn, c in enumerate(kclusters):\n",
      "    print str(c+1) +': '+itememos[cn]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "do pca with dimensions as columns\n",
      "display dimensions with highest loadings on top eigenvectors"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "thresh=.02 #.02% variance explained\n",
      "[dim_eigenvectors, dim_eigenvalues, dim_transformed, dim_evlabels, dim_evvalues]=ndim.myPCA(thresh, np.array(itemavgs), 'PCA on dimensions', dimlabels, figuresize=screesize)\n",
      "[passedvals, passnames]=ndim.eigentable(emolabelmapping,dim_evlabels,dim_evvalues,num=3)\n",
      "# plot in PC space (based on suggested n components)\n",
      "ndim.plotcorrmatrix('item-wise correlations (of tranformed item vectors in dimension PC space)', orderedlabels, dim_transformed, 'RD_items_'+suffix+'.png',figuresize=matrixsize)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "do pca with items as columns\n",
      "display items with highest loadings on top eigenvectors"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#[item_eigenvectors, item_eigenvalues, item_transformed, item_evlabels,item_evvalues]=ndim.myPCA(thresh, np.array(itemavgs).T, 'PCA on items', orderedlabels, figuresize=screesize)\n",
      "#[passedvals, passnames]=ndim.eigentable(emolabelmapping,item_evlabels,item_evvalues,num=3)\n",
      "#ndim.plotcorrmatrix('item-wise correlations (of tranformed dimension vectors in item PC space)', dimlabels, item_transformed, 'RD_items_'+suffix+'.png',figuresize=matrixsize)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "classify emo based on all dimensions\n",
      "cross validate across split halves\n",
      "display emo similarity space in each fold  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cvfolds=range(2)\n",
      "cvtype='half'\n",
      "alldata=[]\n",
      "alllabels=[]\n",
      "for i in cvfolds:\n",
      "    [theseitemavgs,theseitemlabels, thesedimlabels, theseitememos]=ndim.getitemavgs(keepers,orderedlabels, dims, **{cvtype:i+1})\n",
      "    [theseemoavgs, theseemolabels, thesedimlabels]=ndim.getemoavgs(keepers,orderedemos, dims, **{cvtype:i+1})\n",
      "    #[theseitemavgs, theseitemlabels, theseitememos, theseemoavgs, theseemolabels]=reduce2subset(basicsubset,theseitemavgs, theseitemlabels, theseitememos, theseemoavgs, theseemolabels)\n",
      "    [theseorderedlabels, theseorderedemos]=ndim.orderlists(theseemolabels,theseitemlabels,keepers) # reorder in case you lost some emos\n",
      "    ndim.plotcorrmatrix('emo-wise correlations (of avg dimension vectors) in each fold', theseorderedemos, theseemoavgs, 'emos_'+suffix+cvtype+str(i)+'.png', figuresize=smallfig)\n",
      "    alldata.append(theseemoavgs)\n",
      "    alllabels.append(theseemolabels)\n",
      "[classdeets1, accuracies1, chance1]= ndim.classify(cvfolds, alldata, alllabels)\n",
      "crosscorrs=ndim.crossmatrixcorr(alldata)\n",
      "ndim.plotweightmatrix('emo-wise correlations (across halves)', theseemolabels, theseemolabels, crosscorrs, 'emosimilarities_xhalves'+suffix+'.png', figuresize=[7,5],cmin=-1, cmax=1, cmapspec='RdYlBu_r')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "[classdeets1, accuracies1, chance1]= ndim.classify(cvfolds, alldata, alllabels)\n",
      "accuracies1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "classify emo based on dimensions...do things seperately for different subsets of the items. \n",
      "note: do randomized permutations rather than fixed folds based on hitnum...\n",
      "put the following inside a permutation loop and set hitnum=indices[i]\n",
      "display similarity matrices for different folds (comment out the plotting if doing permutation)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#indices=range(5)\n",
      "#shuffle(indices)\n",
      "cvfolds=range(5)\n",
      "cvtype='hitnum'\n",
      "alldata=[]\n",
      "alllabels=[]\n",
      "for i in cvfolds:\n",
      "    [theseitemavgs,theseitemlabels, thesedimlabels, theseitememos]=ndim.getitemavgs(keepers,orderedlabels, dims, **{cvtype:i+1})\n",
      "    [theseemoavgs, theseemolabels, thesedimlabels]=ndim.getemoavgs(keepers,orderedemos, dims, **{cvtype:i+1})\n",
      "    #[theseitemavgs, theseitemlabels, theseitememos, theseemoavgs, theseemolabels]=reduce2subset(basicsubset,theseitemavgs, theseitemlabels, theseitememos, theseemoavgs, theseemolabels)\n",
      "    [theseorderedlabels, theseorderedemos]=ndim.orderlists(theseemolabels,theseitemlabels,keepers) # reorder in case you lost some emos \n",
      "    ndim.plotcorrmatrix('emo-wise correlations (of avg dimension vectors) in each fold', theseorderedemos, theseemoavgs, 'emos_'+suffix+cvtype+str(i)+'.png', figuresize=smallfig)\n",
      "    alldata.append(theseemoavgs)\n",
      "    alllabels.append(theseemolabels)\n",
      "[classdeets2, accuracies2, chance2]= ndim.classify(cvfolds, alldata, alllabels)\n",
      "crosscorrs=ndim.crossmatrixcorr(alldata)\n",
      "ndim.plotweightmatrix('emo-wise correlations (across items)', theseemolabels, theseemolabels, crosscorrs, 'emosimilarities_xitems'+suffix+'.png', figuresize=[7,5], cmin=-1, cmax=1, cmapspec='RdYlBu_r')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "[classdeets2, accuracies2, chance2]= ndim.classify(cvfolds, alldata, alllabels)\n",
      "accuracies=[ac for ac in accuracies1]\n",
      "accuracies.extend(accuracies2)\n",
      "print chance2\n",
      "fig =plt.figure()\n",
      "ax = fig.add_subplot(1,1,1,)\n",
      "plt.bar(range(len(accuracies)), accuracies)\n",
      "plt.plot([0,7],[chance2, chance2],color='r')\n",
      "ax.set_xticks([i+.4 for i in range(7)])\n",
      "ax.set_xticklabels(['split-half1','split-half2','item-cv1','item-cv2','item-cv3','item-cv4','item-cv5'])\n",
      "ax.set_ylim([0,1])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "figure out how to show that you are capitalizing on a higher-d space\n",
      "    -show that eigenvectors explaining the most variance aren't just valence?\n",
      "    -show that you can classify even if excluding valence related dimensions?\n",
      "    -show that you take a hit if reducing to massively lower-d space?\n",
      "    -show that you can classify above chance when analyzing within positive and negative valence separately?"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "compare nlp classification based on appraisals to..\n",
      "    -single level classification based on bag of words\n",
      "    -multilevel algorithm that estimates probabability of appraisals from words, and then emo label from appraisals"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "individiual differences?\n",
      "asd?\n",
      "explain errors in terms of appraisal extraction? e.g. when emotions are ambiguous or there is less agreement is it because people differ in their appraisal judgments or in the appraisal/emotion mappings?"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}